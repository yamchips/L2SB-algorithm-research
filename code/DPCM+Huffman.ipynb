{"cells":[{"cell_type":"markdown","metadata":{"id":"BDTRaVxiFq9l"},"source":["# Read Bonn Data\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3967,"status":"ok","timestamp":1721205185704,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"gO5oRLxXFuj9"},"outputs":[],"source":["import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from heapq import heappush, heappop, heapify\n","from collections import defaultdict, Counter"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15959,"status":"ok","timestamp":1721205201660,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"64fVvxeLFv3m","outputId":"5daddaac-8a35-416f-ef8a-0c6d53fc9708"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1721205201661,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"H5c51CDPFyK9"},"outputs":[],"source":["os.chdir('/content/drive/My Drive/0-Project')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8214,"status":"ok","timestamp":1721205209872,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"SivibC6PF0WN","outputId":"f0e378be-ffda-4c6d-ec8c-99a0a6e012eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["DataFrame for F:\n","  001 002 003  004  005  006  007 008   009 010  ...  091   092  093  094  \\\n","0  34  60  26  -41   13  -15  -24  23  -263  59  ...   99  -131    1  -41   \n","1  33  47  16  -42    6   -2  -27  17  -263  52  ...  114  -153   -4  -41   \n","2  28  38  13  -48   -1    0  -23  10  -261  51  ...  122  -177   -6  -48   \n","3  22  29  12  -48  -13    2  -28  10  -258  46  ...  132  -194  -15  -48   \n","4  21  28  17  -48  -29   -2  -34   7  -258  43  ...  151  -204   -8  -44   \n","\n","   095  096 097  098 099  100  \n","0  -39   45  75   67   5  -45  \n","1  -27   52  72   86   2  -53  \n","2  -16   79  72   99  -6  -51  \n","3   -3  117  80  109  -4  -52  \n","4   17  146  81  115  -9  -54  \n","\n","[5 rows x 100 columns]\n","(4097, 100)\n","\n","DataFrame for O:\n","   001  002  003  004  005  006  007  008  009  010  ...  091   092   093  \\\n","0  -24  -55  -36  -14  -58   87  -52    2    8  -53  ...   62  -128   -83   \n","1  -22  -48  -40   -5  -78   98  -56   -6    0  -15  ...   49  -158  -120   \n","2  -17  -48  -36    0  -83  103  -49  -22  -16   11  ...   14  -161  -123   \n","3  -18  -38  -35    7  -69  108  -51  -17  -38   38  ...  -14  -158  -119   \n","4  -19  -23  -22    7  -31   97  -53    6  -31   35  ...  -14  -152   -93   \n","\n","   094  095   096   097 098  099  100  \n","0   98  -53    29   -76  27   -6  -18  \n","1  178  -33    10  -102   4  -19  -13  \n","2  236  -23   -26   -95   1  -47    0  \n","3  237    0   -81   -65  19  -83    9  \n","4  187   10  -127   -30  43  -99    6  \n","\n","[5 rows x 100 columns]\n","(4097, 100)\n","\n","DataFrame for S:\n","   001  002   003   004  005   006  007  008   009    010  ...   091  092  \\\n","0  100  340  -310    84  343   -88  176  661  -377    374  ...  -129  -26   \n","1  124  353    93    75  311  -115  186  721  -379   -205  ...  -309    1   \n","2  153  400   494    21  284  -140  189  702  -396   -871  ...  -432   29   \n","3  185  470   789   -68  274  -159  198  628  -448  -1325  ...  -412   41   \n","4  210  538   798  -138  260  -164  205  519  -476  -1291  ...  -278   33   \n","\n","   093   094   095  096   097   098   099  100  \n","0  308  -155  -113  -40   187  -438  -476   23  \n","1  367  -283  -185  -58    44  -561  -518  144  \n","2  413  -456  -269  -75  -147  -622  -521  228  \n","3  429  -541  -328  -88  -368  -581  -362  260  \n","4  400  -474  -312  -89  -550  -460   -68  255  \n","\n","[5 rows x 100 columns]\n","(4097, 100)\n"]}],"source":["folders = ['F', 'O', 'S']\n","dataframes = {}\n","\n","for folder_path in folders:\n","  file_list = os.listdir(folder_path)\n","  data_dict = {}\n","\n","  for file_name in file_list:\n","    column_name = file_name[1:4]\n","    file_path = os.path.join(folder_path, file_name)\n","    with open(file_path, 'r') as file:\n","      file_data = [line.strip() for line in file.readlines()]\n","      data_dict[column_name] = file_data\n","\n","  df = pd.DataFrame(data_dict)\n","  df = df[sorted(df.columns)]\n","\n","  # Store the DataFrame in the dictionary with the folder name as the key\n","  dataframes[folder_path] = df\n","\n","# Accessing the DataFrames for F, O, and S\n","df_F = dataframes['F']\n","df_O = dataframes['O']\n","df_S = dataframes['S']\n","\n","# Print head and shape of each DataFrame\n","print(\"DataFrame for F:\")\n","print(df_F.head())\n","print(df_F.shape)  # Should be (4097, 100)\n","\n","print(\"\\nDataFrame for O:\")\n","print(df_O.head())\n","print(df_O.shape)  # Should be (4097, 100)\n","\n","print(\"\\nDataFrame for S:\")\n","print(df_S.head())\n","print(df_S.shape)  # Should be (4097, 100)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1181,"status":"ok","timestamp":1721205211050,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"zo98CJeGF5W2"},"outputs":[],"source":["df_F = df_F.apply(pd.to_numeric)\n","df_O = df_O.apply(pd.to_numeric)\n","df_S = df_S.apply(pd.to_numeric)"]},{"cell_type":"markdown","metadata":{"id":"zFAPB2mNEvKd"},"source":["# Implement DPCM and Huffman compression algorithm\n","\n","Example:\n","\n","We have a series of data:\n","\n","100 70 120 40 50 80\n","\n","We calculate the difference between them (DPCM):\n","\n","100 -30 50 -80 10 30\n","\n","Then for all these new data, we use Huffman encoding to compress them\n"]},{"cell_type":"markdown","metadata":{"id":"rSRavRgkIqYF"},"source":["Since we are using the difference, we don't have to add offset to original data.\n","\n","When we are dealing with a dataframe, should we flatten the dataframe into a list or we preserve the value of first row for each column?\n","\n","Each column represents a sample of data series. If we flatten the data into a list, we assume that all data form a series. But that's not the case. So we preserve the value of first row."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721205535729,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"fUY6LuCTEuRD"},"outputs":[],"source":["def DPCM(data):\n","  '''\n","  Params:\n","    data : input data frame\n","  For each column, calculate the difference between i-th row and i+1-th row,\n","  use this value to represent the data in i+1-th row.\n","  This function preserves the value in first row of each column.\n","  Returns a new dataframe.\n","  '''\n","  temp = data.copy()\n","  for col in temp.columns:\n","    diff_res = temp[col].diff()\n","    diff_res.iloc[0] = temp[col].iloc[0]\n","    temp[col] = diff_res\n","  return temp"]},{"cell_type":"markdown","metadata":{"id":"_wsKANSlOjgW"},"source":["When doing Huffman coding, we can flatten the dataframe into a list. In reality we might need to compress and send many data samples at a time, these samples are not from one series. If we calculate one Huffman tree for each column, then we need many frequency lists, which is not efficient."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1721207924712,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"SljkErVZPkn3"},"outputs":[],"source":["class HuffmanNode:\n","  '''\n","  A Huffman tree node.\n","  '''\n","  def __init__(self, value=None, frequency=None, left=None, right=None):\n","    self.value = value\n","    self.frequency = frequency\n","    self.left = left\n","    self.right = right\n","\n","  # Comparison methods for the priority queue\n","  def __lt__(self, other):\n","    return self.frequency < other.frequency\n","\n","def build_huffman_tree(frequency):\n","  '''\n","  Params:\n","    frequency : dictionary of frequencies\n","  Returns the root of a Huffman tree.\n","  '''\n","  heap = [HuffmanNode(value, freq) for value, freq in frequency.items()]\n","  heapify(heap)\n","\n","  while len(heap) > 1:\n","    left = heappop(heap)\n","    right = heappop(heap)\n","    merged = HuffmanNode(None, left.frequency + right.frequency, left, right)\n","    heappush(heap, merged)\n","\n","  return heap[0]\n","\n","def generate_codes(node, prefix=\"\", codebook={}):\n","  '''\n","  Params:\n","    node : Huffman tree node\n","    prefix : prefix code\n","    codebook : dictionary of codes\n","  Returns a dictionary of codes.\n","  '''\n","  if node is not None:\n","    if node.value is not None:\n","      codebook[node.value] = prefix\n","    generate_codes(node.left, prefix + \"0\", codebook)\n","    generate_codes(node.right, prefix + \"1\", codebook)\n","  return codebook\n","\n","def huffman_encoding(data):\n","  '''\n","  Params:\n","    data : input data frame\n","  Returns a tuple of encoded data and Huffman codes.\n","  '''\n","  # flatten the DataFrame and calculate frequencies\n","  values = data.values.flatten()\n","  frequency = Counter(values)\n","\n","  # build Huffman Tree\n","  huffman_tree = build_huffman_tree(frequency)\n","\n","  # generate Huffman Codes\n","  huffman_codes = generate_codes(huffman_tree)\n","\n","  # encode the dataFrame\n","  encoded_data = data.applymap(lambda x: huffman_codes[x])\n","\n","  return encoded_data, huffman_codes"]},{"cell_type":"markdown","metadata":{"id":"xdvWgDpN-d1b"},"source":["#Test case"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1721207929100,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"DbgbK8yCZzcK","outputId":"5f1dd6b1-0b7a-4e83-be44-d996610bd2b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoded Data:\n","    A    B    C\n","0   0    0    0\n","1   0  111  111\n","2  10   10   10\n","3   0    0  110\n","\n","Huffman Codes:\n","{1: '0', 3: '10', 4: '110', 2: '111'}\n"]}],"source":["# Test huffman tree\n","data = pd.DataFrame({\n","    'A': [1, 1, 3, 1],\n","    'B': [1, 2, 3, 1],\n","    'C': [1, 2, 3, 4]\n","})\n","\n","encoded_data, huffman_codes = huffman_encoding(data)\n","\n","print(\"Encoded Data:\")\n","print(encoded_data)\n","print(\"\\nHuffman Codes:\")\n","print(huffman_codes)"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721209252359,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"ZPCk4uF8BsZX"},"outputs":[],"source":["# iterate all elements of encoded_data and get the total length\n","total_length = 0\n","for col in encoded_data.columns:\n","  for val in encoded_data[col]:\n","    total_length += len(val)\n"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721209374038,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"ZSHdUMGOEdnR","outputId":"71b36d45-94c3-4624-f4be-bce75c6d8d50"},"outputs":[{"data":{"text/plain":["21"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["encoded_data.astype(str).applymap(len).values.sum()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1721207725945,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"qf10dyecapHC","outputId":"d4d5b0ba-75f5-4365-858d-94e7c284c27d"},"outputs":[{"name":"stdout","output_type":"stream","text":["     1    2     3     4\n","0  1.0  5.0   9.0  10.0\n","1  6.0  4.0  11.0   0.0\n","2 -4.0 -2.0  -9.0   9.0\n","3  1.0  1.0   1.0  -3.0\n"]}],"source":["# Test DPCM\n","test_df = pd.DataFrame({\n","    '1':[1, 7, 3, 4],\n","    '2':[5, 9, 7, 8],\n","    '3':[9, 20, 11, 12],\n","    '4':[10, 10, 19, 16]\n","})\n","\n","dpcm_df = DPCM(test_df)\n","print(dpcm_df)"]},{"cell_type":"markdown","metadata":{"id":"7TLCxWhL-f-a"},"source":["# Compress Bonn data"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3442,"status":"ok","timestamp":1721207935614,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"U5O77J7h-iEz"},"outputs":[],"source":["f_dpcm = DPCM(df_F)\n","o_dpcm = DPCM(df_O)\n","s_dpcm = DPCM(df_S)\n","f_encoded, f_codes = huffman_encoding(f_dpcm)\n","o_encoded, o_codes = huffman_encoding(o_dpcm)\n","s_encoded, s_codes = huffman_encoding(s_dpcm)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1620,"status":"ok","timestamp":1721209481352,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"XT921uph_PYi","outputId":"154219fe-6138-4b65-9460-d246919c56cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Compression ratio for F: 2.129795169015994\n","Compression ratio for O: 1.7762407744878008\n","Compression ratio for S: 1.397969867044433\n"]}],"source":["f_length = f_encoded.astype(str).applymap(len).values.sum()\n","o_length = o_encoded.astype(str).applymap(len).values.sum()\n","s_length = s_encoded.astype(str).applymap(len).values.sum()\n","\n","cr_f = 4097*100*12 / f_length\n","cr_o = 4097*100*12 / o_length\n","cr_s = 4097*100*12 / s_length\n","print(f'Compression ratio for F: {cr_f}')\n","print(f'Compression ratio for O: {cr_o}')\n","print(f'Compression ratio for S: {cr_s}')"]},{"cell_type":"markdown","metadata":{"id":"qISziCFdFQKO"},"source":["dataset |  my result\n","----|---\n","F |  2.13\n","O |  1.78\n","S |  1.4\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9_tKgFkLGLq_"},"source":["# Import and compress MIT data"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":263,"status":"ok","timestamp":1721209832290,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"t5ZD5agLGO3D"},"outputs":[],"source":["os.chdir('/content/drive/My Drive/0-Project/MIT')"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":14454,"status":"ok","timestamp":1721209847872,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"Wayn52UZGVp0"},"outputs":[],"source":["# 48 csv files\n","csv_files = [f for f in os.listdir() if f.endswith('.csv')]\n","\n","df_mix = pd.DataFrame()\n","\n","for file in csv_files:\n","  file_path = os.path.join(os.getcwd(), file)\n","\n","  df = pd.read_csv(file_path, skiprows=1)\n","\n","  df_last_two_cols = df.iloc[:, -2:]\n","\n","  file_prefix = file[:3]\n","  df_last_two_cols.columns = [f\"{file_prefix}-1\", f\"{file_prefix}-2\"]\n","\n","  df_mix = pd.concat([df_mix, df_last_two_cols], axis=1)\n"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1721211673275,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"cbO1_nX6NO7D"},"outputs":[],"source":["def transfer(data):\n","  '''\n","  Params:\n","    data : input data frame\n","  First we add offset to input data frame so we have a dataset of non-negative numbers.\n","  Transfer the data using following equation:\n","  new_data = 4095 * data[i] / (max - min)\n","  '''\n","  new_data = data.copy()\n","  min = data.min().min()\n","  if min < 0:\n","    new_data -= min\n","  min = new_data.min().min()\n","  max = new_data.max().max()\n","  new_data = 4095 * new_data / (max - min)\n","  new_data = new_data.round().astype(int)\n","  return new_data"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721211713551,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"GdXXZhpSKQGV"},"outputs":[],"source":["# first we scale all data\n","# df_mix_scaled = df_mix.mul(600)\n","# round the scaled result into integers\n","# df_mix_scaled = df_mix_scaled.round().astype(int)\n","# scaled_min = df_mix_scaled.min().min()\n","# # add offset to all data, make them non-negative\n","# df_mix_scaled = df_mix_scaled.add(abs(scaled_min))\n","\n","df_mix_scaled = transfer(df_mix)\n","\n"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":273,"status":"ok","timestamp":1721211717213,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"rLDteMoWKHLS"},"outputs":[],"source":["# choose 1,3,5.. columns in df_mixed_scaled as mixed_signal_1\n","mixed_signal_1 = df_mix_scaled.iloc[:,1::2]\n","# choose 0,2,4.. columns in df_mixed_scaled as mixed_signal_2\n","mixed_signal_2 = df_mix_scaled.iloc[:,::2]"]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721211722057,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"ZoHiuZe7KXiW"},"outputs":[],"source":["mixed_1_dpcm = DPCM(mixed_signal_1)\n","mixed_2_dpcm = DPCM(mixed_signal_2)"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721211725245,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"CDHEkF6VKfSH"},"outputs":[],"source":["mixed_1_encoded, mixed_1_codes = huffman_encoding(mixed_1_dpcm)\n","mixed_2_encoded, mixed_2_codes = huffman_encoding(mixed_2_dpcm)\n"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1721211729088,"user":{"displayName":"Fan Wu","userId":"10431887109479189055"},"user_tz":-60},"id":"mMfIZCXUKsr6","outputId":"64185b08-40f6-46e6-96d5-d1d425cf7712"},"outputs":[{"name":"stdout","output_type":"stream","text":["Compression ratio for mixed_signal_1: 2.4353463462758085\n","Compression ratio for mixed_signal_2: 2.347834061936352\n"]}],"source":["mixed_1_length = mixed_1_encoded.astype(str).applymap(len).values.sum()\n","mixed_2_length = mixed_2_encoded.astype(str).applymap(len).values.sum()\n","cr_mixed_1 = 3600*48*12 / mixed_1_length\n","cr_mixed_2 = 3600*48*12 / mixed_2_length\n","print(f'Compression ratio for mixed_signal_1: {cr_mixed_1}')\n","print(f'Compression ratio for mixed_signal_2: {cr_mixed_2}')"]},{"cell_type":"markdown","metadata":{"id":"B0We0smPLNcI"},"source":["dataset | my result using scaling | my result using transfer\n","--|--|--\n","mixed signal 1 | 2.819 | 2.435\n","mixed signal 2 | 2.697 | 2.348\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPBG8/7ciFr8FyJxjX3jLUx","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
